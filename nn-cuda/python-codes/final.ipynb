{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "        \n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float()\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float()\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float()\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "    \n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "print(X_train [0,0:5])\n",
    "# X_test = np.load('../dataset/x_test.npy')\n",
    "# y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y1, y1_relu, y2 = model(X)\n",
    "        loss = criterion(y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float())\n",
    "y_pred = y2.argmax(dim=1).numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "        \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float()\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float()\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float()\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "    \n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    with profiler.profile(record_shapes=True, use_cuda=False) as prof:\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y1, y1_relu, y2 = model(X)\n",
    "            loss = criterion(y2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch: {epoch+1}, loss: {loss.item()}\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float())\n",
    "y_pred = y2.argmax(dim=1).numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float().to(device)\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float().to(device)\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float().to(device)\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "# Load data and move to device\n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "print(X_train[0, 0:5])\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train).float().to(device), \n",
    "                                        torch.from_numpy(y_train).long().to(device)), \n",
    "                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y1, y1_relu, y2 = model(X)\n",
    "        loss = criterion(y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Testing the model\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float().to(device))\n",
    "y_pred = y2.argmax(dim=1).cpu().numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[0.78198105 0.4786313  0.7654065  0.05289226 0.815995  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.9396262168884277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Print profiling results\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_averages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Testing the model\u001b[39;00m\n\u001b[0;32m     75\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/x_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler.py:365\u001b[0m, in \u001b[0;36mprofile.key_averages\u001b[1;34m(self, group_by_input_shape, group_by_stack_n)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_finish()\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected profiling results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_averages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_by_input_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_by_stack_n\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler_util.py:324\u001b[0m, in \u001b[0;36mEventList.key_averages\u001b[1;34m(self, group_by_input_shapes, group_by_stack_n)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(key)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_by_input_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_by_stack_n\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m avg_list \u001b[38;5;241m=\u001b[39m EventList(\n\u001b[0;32m    327\u001b[0m     stats\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    328\u001b[0m     use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_cuda,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m     with_flops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_flops,\n\u001b[0;32m    332\u001b[0m )\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evt \u001b[38;5;129;01min\u001b[39;00m avg_list:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler_util.py:708\u001b[0m, in \u001b[0;36mFunctionEventAvg.add\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m other\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_time_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mcpu_time_total\n\u001b[1;32m--> 708\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_time_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_time_total\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivateuse1_time_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mprivateuse1_time_total\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_cpu_time_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mself_cpu_time_total\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler_util.py:556\u001b[0m, in \u001b[0;36mFunctionEvent.cuda_time_total\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m==\u001b[39m DeviceType\u001b[38;5;241m.\u001b[39mCPU:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_legacy:\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;66;03m# account for the kernels in the children ops\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkinfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m    557\u001b[0m             ch\u001b[38;5;241m.\u001b[39mcuda_time_total \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_children\n\u001b[0;32m    558\u001b[0m         )\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;66;03m# each legacy cpu events has a single (fake) kernel\u001b[39;00m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(kinfo\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;28;01mfor\u001b[39;00m kinfo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float().to(device)\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float().to(device)\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float().to(device)\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "# Load data and move to device\n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "print(X_train[0, 0:5])\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train).float().to(device), \n",
    "                                        torch.from_numpy(y_train).long().to(device)), \n",
    "                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    with profiler.profile(record_shapes=True) as prof:\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y1, y1_relu, y2 = model(X)\n",
    "            loss = criterion(y2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    # Print profiling results\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "# Testing the model\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float().to(device))\n",
    "y_pred = y2.argmax(dim=1).cpu().numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
