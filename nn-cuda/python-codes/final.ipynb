{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "        \n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float()\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float()\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float()\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "    \n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "print(X_train [0,0:5])\n",
    "# X_test = np.load('../dataset/x_test.npy')\n",
    "# y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y1, y1_relu, y2 = model(X)\n",
    "        loss = criterion(y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float())\n",
    "y_pred = y2.argmax(dim=1).numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "        \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float()\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float()\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float()\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "    \n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model-cpu')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    with profiler.profile(record_shapes=True, use_cuda=False) as prof:\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y1, y1_relu, y2 = model(X)\n",
    "            loss = criterion(y2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch: {epoch+1}, loss: {loss.item()}\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float())\n",
    "y_pred = y2.argmax(dim=1).numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[ 1.2802037   0.1930291   0.14813042 -0.9187077   0.39928705]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aatta\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 17.527576446533203\n",
      "Time taken: 0.7082126140594482\n",
      "Epoch: 2, Loss: 11.733003616333008\n",
      "Time taken: 0.3717939853668213\n",
      "Epoch: 3, Loss: 9.270295143127441\n",
      "Time taken: 0.3654367923736572\n",
      "Epoch: 4, Loss: 7.723914623260498\n",
      "Time taken: 0.3745746612548828\n",
      "Epoch: 5, Loss: 6.6460981369018555\n",
      "Time taken: 0.47765040397644043\n",
      "Epoch: 6, Loss: 5.852162837982178\n",
      "Time taken: 0.4715690612792969\n",
      "Epoch: 7, Loss: 5.230144500732422\n",
      "Time taken: 0.4310479164123535\n",
      "Epoch: 8, Loss: 4.732577323913574\n",
      "Time taken: 0.39620208740234375\n",
      "Epoch: 9, Loss: 4.3370442390441895\n",
      "Time taken: 0.40636658668518066\n",
      "Epoch: 10, Loss: 4.010096549987793\n",
      "Time taken: 0.40944695472717285\n",
      "Epoch: 11, Loss: 3.7300851345062256\n",
      "Time taken: 0.3954811096191406\n",
      "Epoch: 12, Loss: 3.4881057739257812\n",
      "Time taken: 0.3741786479949951\n",
      "Epoch: 13, Loss: 3.283463478088379\n",
      "Time taken: 0.4808011054992676\n",
      "Epoch: 14, Loss: 3.105936288833618\n",
      "Time taken: 0.40611791610717773\n",
      "Epoch: 15, Loss: 2.9462108612060547\n",
      "Time taken: 0.3911290168762207\n",
      "Epoch: 16, Loss: 2.7978906631469727\n",
      "Time taken: 0.529900074005127\n",
      "Epoch: 17, Loss: 2.6607627868652344\n",
      "Time taken: 0.3580453395843506\n",
      "Epoch: 18, Loss: 2.5370519161224365\n",
      "Time taken: 0.37363576889038086\n",
      "Epoch: 19, Loss: 2.42417049407959\n",
      "Time taken: 0.36171770095825195\n",
      "Epoch: 20, Loss: 2.3201212882995605\n",
      "Time taken: 0.35342836380004883\n",
      "Total time taken: 8.436736106872559\n",
      "Average time per epoch: 0.42183680534362794 seconds\n",
      "Accuracy: 0.09025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "import time\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float().to(device)\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float().to(device)\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float().to(device)\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "# Load data and move to device\n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "print(X_train[0, 0:5])\n",
    "\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train).float(), \n",
    "                                        torch.from_numpy(y_train).long()), \n",
    "                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "total_time = 0.0\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y1, y1_relu, y2 = model(X)\n",
    "        loss = criterion(y2, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    print(f\"Time taken: {time.time() - start_time}\")\n",
    "    total_time += time.time() - start_time\n",
    "print(f\"Total time taken: {total_time}\")\n",
    "print(f\"Average time per epoch: {total_time / N_EPOCHS} seconds\" )\n",
    "# Testing the model\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float().to(device))\n",
    "y_pred = y2.argmax(dim=1).cpu().numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[ 1.2802037   0.1930291   0.14813042 -0.9187077   0.39928705]\n",
      "Epoch: 1, Loss: 33.8510627746582\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        33.12%        1.251s        97.54%        3.683s     111.612ms     713.047ms        18.80%        3.682s     111.562ms            33  \n",
      "                                           aten::select        32.75%        1.237s        33.02%        1.247s      19.484us        1.233s        32.49%        1.783s      27.860us         64000  \n",
      "                                            aten::stack        15.82%     597.520ms        31.39%        1.185s      18.523ms     361.259ms         9.52%        1.185s      18.523ms            64  \n",
      "                                        aten::unsqueeze        15.15%     572.222ms        15.28%     576.897ms      18.028us     532.222ms        14.03%     812.687ms      25.396us         32000  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.10%       3.706ms         0.53%      20.162ms     315.031us       1.995ms         0.05%      25.498ms     398.406us            64  \n",
      "                                               aten::to         0.03%     996.000us         0.48%      17.949ms     186.969us       1.071ms         0.03%      18.304ms     190.667us            96  \n",
      "                                         aten::_to_copy         0.08%       3.059ms         0.45%      16.953ms     264.891us       2.664ms         0.07%      17.233ms     269.266us            64  \n",
      "                                       aten::as_strided         0.41%      15.640ms         0.41%      15.640ms       0.162us     833.187ms        21.96%     833.187ms       8.644us         96384  \n",
      "                                         AddmmBackward0         0.10%       3.668ms         0.37%      13.854ms     216.469us       3.046ms         0.08%      19.569ms     305.766us            64  \n",
      "                                            aten::copy_         0.35%      13.154ms         0.35%      13.154ms     205.531us      13.488ms         0.36%      13.488ms     210.750us            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.776s\n",
      "Self CUDA time total: 3.793s\n",
      "\n",
      "Epoch: 2, Loss: 24.480039596557617\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        34.66%        1.296s        97.49%        3.646s     110.492ms     825.659ms        22.00%        3.645s     110.456ms            33  \n",
      "                                           aten::select        31.86%        1.192s        32.07%        1.200s      18.744us        1.179s        31.40%        1.669s      26.076us         64000  \n",
      "                                            aten::stack        15.62%     584.388ms        30.75%        1.150s      17.973ms     350.506ms         9.34%        1.151s      17.978ms            64  \n",
      "                                        aten::unsqueeze        14.68%     549.203ms        14.78%     552.945ms      17.280us     539.821ms        14.38%     786.269ms      24.571us         32000  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.09%       3.472ms         0.51%      19.060ms     297.812us       1.750ms         0.05%      23.455ms     366.484us            64  \n",
      "                                               aten::to         0.03%       1.026ms         0.47%      17.667ms     184.031us       1.304ms         0.03%      18.244ms     190.042us            96  \n",
      "                                         aten::_to_copy         0.06%       2.417ms         0.44%      16.641ms     260.016us       1.806ms         0.05%      16.940ms     264.688us            64  \n",
      "                                            aten::copy_         0.36%      13.474ms         0.36%      13.474ms     210.531us      13.942ms         0.37%      13.942ms     217.844us            64  \n",
      "                                         AddmmBackward0         0.10%       3.911ms         0.36%      13.372ms     208.938us       2.932ms         0.08%      18.106ms     282.906us            64  \n",
      "                                           aten::linear         0.05%       1.951ms         0.36%      13.290ms     207.656us       1.241ms         0.03%      18.391ms     287.359us            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.740s\n",
      "Self CUDA time total: 3.754s\n",
      "\n",
      "Epoch: 3, Loss: 19.59640121459961\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        34.98%        1.257s        97.45%        3.501s     106.089ms     813.724ms        22.56%        3.500s     106.059ms            33  \n",
      "                                           aten::select        31.27%        1.123s        31.44%        1.130s      17.649us        1.108s        30.72%        1.572s      24.556us         64000  \n",
      "                                            aten::stack        15.49%     556.312ms        31.03%        1.115s      17.419ms     332.058ms         9.20%        1.115s      17.417ms            64  \n",
      "                                        aten::unsqueeze        15.13%     543.396ms        15.25%     547.770ms      17.118us     526.207ms        14.59%     770.812ms      24.088us         32000  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.09%       3.303ms         0.55%      19.589ms     306.078us       2.051ms         0.06%      23.513ms     367.391us            64  \n",
      "                                               aten::to         0.03%       1.109ms         0.44%      15.873ms     165.344us       1.331ms         0.04%      16.345ms     170.260us            96  \n",
      "                                         aten::_to_copy         0.07%       2.572ms         0.41%      14.764ms     230.688us       1.944ms         0.05%      15.014ms     234.594us            64  \n",
      "                                         AddmmBackward0         0.11%       4.069ms         0.38%      13.775ms     215.234us       2.919ms         0.08%      17.598ms     274.969us            64  \n",
      "                                           aten::linear         0.06%       1.976ms         0.36%      13.085ms     204.453us       1.404ms         0.04%      18.558ms     289.969us            64  \n",
      "                                            aten::copy_         0.32%      11.405ms         0.32%      11.405ms     178.203us      11.901ms         0.33%      11.901ms     185.953us            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.592s\n",
      "Self CUDA time total: 3.607s\n",
      "\n",
      "Epoch: 4, Loss: 16.475894927978516\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        35.29%        1.247s        97.47%        3.443s     104.336ms     813.787ms        22.95%        3.442s     104.309ms            33  \n",
      "                                           aten::select        31.07%        1.097s        31.22%        1.103s      17.229us        1.093s        30.83%        1.535s      23.978us         64000  \n",
      "                                            aten::stack        15.60%     551.076ms        30.96%        1.094s      17.089ms     331.929ms         9.36%        1.094s      17.091ms            64  \n",
      "                                        aten::unsqueeze        14.99%     529.518ms        15.05%     531.782ms      16.618us     516.312ms        14.56%     750.369ms      23.449us         32000  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.10%       3.394ms         0.54%      19.052ms     297.688us       1.848ms         0.05%      23.830ms     372.344us            64  \n",
      "                                               aten::to         0.03%       1.158ms         0.45%      15.819ms     164.781us       1.390ms         0.04%      16.290ms     169.688us            96  \n",
      "                                         aten::_to_copy         0.07%       2.463ms         0.42%      14.661ms     229.078us       1.909ms         0.05%      14.900ms     232.812us            64  \n",
      "                                         AddmmBackward0         0.11%       4.032ms         0.38%      13.534ms     211.469us       3.185ms         0.09%      17.976ms     280.875us            64  \n",
      "                                           aten::linear         0.06%       2.125ms         0.37%      13.217ms     206.516us       1.363ms         0.04%      18.122ms     283.156us            64  \n",
      "                                            aten::copy_         0.32%      11.401ms         0.32%      11.401ms     178.141us      11.795ms         0.33%      11.795ms     184.297us            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.532s\n",
      "Self CUDA time total: 3.546s\n",
      "\n",
      "Epoch: 5, Loss: 14.437370300292969\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        35.02%        1.295s        97.63%        3.611s     109.422ms     838.362ms        22.58%        3.611s     109.438ms            33  \n",
      "                                           aten::select        32.49%        1.201s        32.88%        1.216s      19.000us        1.182s        31.84%        1.674s      26.161us         64000  \n",
      "                                            aten::stack        15.12%     559.099ms        29.73%        1.100s      17.181ms     332.110ms         8.94%        1.099s      17.169ms            64  \n",
      "                                        aten::unsqueeze        14.25%     526.903ms        14.33%     529.975ms      16.562us     519.565ms        13.99%     755.950ms      23.623us         32000  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.10%       3.740ms         0.52%      19.257ms     300.891us       1.825ms         0.05%      22.801ms     356.266us            64  \n",
      "                                       aten::as_strided         0.48%      17.937ms         0.48%      17.937ms       0.186us     731.414ms        19.70%     731.414ms       7.589us         96384  \n",
      "                                               aten::to         0.02%     916.000us         0.41%      15.072ms     157.000us       1.117ms         0.03%      15.332ms     159.708us            96  \n",
      "                                         aten::_to_copy         0.06%       2.313ms         0.38%      14.156ms     221.188us       1.743ms         0.05%      14.215ms     222.109us            64  \n",
      "                                         AddmmBackward0         0.11%       3.947ms         0.37%      13.565ms     211.953us       2.733ms         0.07%      17.263ms     269.734us            64  \n",
      "                                           aten::linear         0.05%       1.962ms         0.35%      12.815ms     200.234us       1.223ms         0.03%      17.552ms     274.250us            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.698s\n",
      "Self CUDA time total: 3.713s\n",
      "\n",
      "Accuracy: 0.070625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ParamsDownloader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.ln1b = np.load(f'{self.data_path}/ln1b.npy')\n",
    "        self.ln1w = np.load(f'{self.data_path}/ln1w.npy')\n",
    "        self.ln2b = np.load(f'{self.data_path}/ln2b.npy')\n",
    "        self.ln2w = np.load(f'{self.data_path}/ln2w.npy')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ln1w, ln1b, ln2w, ln2b):\n",
    "        super(Model, self).__init__()\n",
    "        N, H1 = ln1w.shape\n",
    "        H2, _ = ln2w.shape\n",
    "        \n",
    "        self.l1 = nn.Linear(N, H1)\n",
    "        self.l1.weight.data = torch.from_numpy(ln1w).float().to(device)\n",
    "        self.l1.bias.data = torch.from_numpy(ln1b).float().to(device)\n",
    "\n",
    "        self.l2 = nn.Linear(H1, H2)\n",
    "        self.l2.weight.data = torch.from_numpy(ln2w).float().to(device)\n",
    "        self.l2.bias.data = torch.from_numpy(ln2b).float().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.l1(x)\n",
    "        y1_relu = F.relu(y1)\n",
    "        y2 = self.l2(y1_relu)\n",
    "        return y1, y1_relu, y2\n",
    "    \n",
    "# Load data and move to device\n",
    "X_train = np.load('../dataset/x_train.npy')\n",
    "y_train = np.load('../dataset/y_train.npy').astype(np.int64)\n",
    "\n",
    "print(X_train[0, 0:5])\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train).float(), \n",
    "                                        torch.from_numpy(y_train).long()), \n",
    "                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loader = ParamsDownloader('../with-torch-tests/trained-model')\n",
    "model = Model(data_loader.ln1w, data_loader.ln1b, data_loader.ln2w, data_loader.ln2b).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    with profiler.profile(record_shapes=True,use_cuda=True) as prof:\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y1, y1_relu, y2 = model(X)\n",
    "            loss = criterion(y2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    # Print profiling results\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# Testing the model\n",
    "x_test = np.load('../dataset/x_test.npy')\n",
    "y_test = np.load('../dataset/y_test.npy')\n",
    "\n",
    "y1, y1_relu, y2 = model(torch.from_numpy(x_test).float().to(device))\n",
    "y_pred = y2.argmax(dim=1).cpu().numpy()\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
