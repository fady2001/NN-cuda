{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00157026 0.9984297 ] [0.40329954 0.59670043 0.        ] [2.1250953e-05 9.9997878e-01 0.0000000e+00]\n",
      "[0.         0.00157026 0.9984297 ] [0.40329954 0.59670043 0.        ] [2.1250953e-05 9.9997878e-01 0.0000000e+00]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# read arrays from .npy files and func Linear to compare\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inp = np.load('../with-torch-tests/softmax-layer/h_inp.npy')\n",
    "out = np.load('../with-torch-tests/softmax-layer/h_out.npy')\n",
    "\n",
    "#apply softmax with torch\n",
    "inp_torch = torch.from_numpy(inp).to(torch.float32)\n",
    "out_torch = nn.functional.softmax(inp_torch, dim=1)\n",
    " \n",
    "print(*out)\n",
    "print(*out_torch.detach().numpy())\n",
    "print( np.allclose(out, out_torch.detach().numpy(), atol=1e-4, rtol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 1])\n",
      "[1.5712201e-03 9.0807462e-01 2.0027361e-05]\n",
      "tensor(0.9097, grad_fn=<NllLossBackward0>)\n",
      "False\n",
      "[ 0.          0.00157026 -0.00157028] [-0.59670043  0.59670043  0.        ] [ 2.1250959e-05 -2.1219254e-05  0.0000000e+00]\n",
      "[ 0.          0.00157026 -0.00157028] [-0.59670043  0.59670043  0.        ] [ 2.1250953e-05 -2.1219254e-05  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# read arrays from .npy files and func Linear to compare\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inp = np.load('../with-torch-tests/softmax-layer/h_inp.npy')\n",
    "target = np.load('../with-torch-tests/cross-entropy-layer/h_targets.npy')\n",
    "out = np.load('../with-torch-tests/cross-entropy-layer/h_losses.npy')\n",
    "\n",
    "#apply softmax with torch\n",
    "inp_torch = torch.from_numpy(inp).to(torch.float32).requires_grad_(True)\n",
    "target = torch.from_numpy(target).to(torch.int64)\n",
    "print(target)\n",
    "out_torch = F.cross_entropy(inp_torch,target,reduction='sum')\n",
    " \n",
    "print(out)\n",
    "print(out_torch)\n",
    "print( np.allclose(out, out_torch.detach().numpy(), atol=1e-4, rtol=1e-4))\n",
    "\n",
    "# backpropagation\n",
    "out_torch.backward()\n",
    "print(*inp_torch.grad.detach().numpy())\n",
    "\n",
    "# read arrays from .npy files and func Linear to compare\n",
    "grad = np.load('../with-torch-tests/cross-entropy-backward/h_dlogits_after.npy')\n",
    "print(*grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
