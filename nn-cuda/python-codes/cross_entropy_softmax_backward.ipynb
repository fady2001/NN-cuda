{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0281,  0.0416,  0.0497, -0.2930,  0.1736],\n",
      "        [ 0.0384,  0.1314,  0.0290,  0.1127, -0.3116],\n",
      "        [ 0.0245,  0.0842,  0.0635, -0.2918,  0.1196]])\n"
     ]
    }
   ],
   "source": [
    "# Relu forward and backward tests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load data\n",
    "X = np.load(r'../with-torch-tests/relu-layer/X_relu.npy')\n",
    "Y = np.load(r'../with-torch-tests/relu-layer/out_relu.npy')\n",
    "dY = np.load(r'../with-torch-tests/relu-layer/up_grad_relu.npy')\n",
    "dX = np.load(r'../with-torch-tests/relu-layer/down_grad_relu.npy')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# make sure to set requires_grad=True for the input tensor so that the Autograd engine can compute the gradients\n",
    "X_torch = torch.from_numpy(X).to(torch.float32).requires_grad_(True)\n",
    "dY_torch = torch.from_numpy(dY).to(torch.float32)\n",
    "\n",
    "# Forward pass with ReLU\n",
    "loss = nn.CrossEntropyLoss()\n",
    "Y_torch = loss(X_torch)\n",
    "\n",
    "# Compare the forward pass results\n",
    "print(\"Forward pass comparison:\")\n",
    "print(\"Y (numpy):\", Y[0, 0:5])\n",
    "print(\"Y_torch:\", Y_torch.detach().numpy()[0, 0:5])\n",
    "print(\"Match:\", np.allclose(Y, Y_torch.detach().numpy(), atol=1e-4, rtol=1e-4))\n",
    "\n",
    "\n",
    "# Validate the backward pass\n",
    "Y_torch.backward(dY_torch)\n",
    "\n",
    "# Get the gradients from X_torch\n",
    "dX_torch = X_torch.grad\n",
    "\n",
    "# Compare the backward pass results\n",
    "print(\"\\nBackward pass comparison:\")\n",
    "print(\"dX (numpy):\", dX[0, 0:5])\n",
    "print(\"dX_torch:\", dX_torch.numpy()[0, 0:5])\n",
    "print(\"Match:\", np.allclose(dX, dX_torch.numpy(), atol=1e-4, rtol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients - Custom Implementation:\n",
      "tensor([[ 0.1673, -0.3337,  0.1664],\n",
      "        [-0.3319,  0.1663,  0.1656]])\n",
      "\n",
      "Gradients - PyTorch Autograd:\n",
      "[[ 0.1672683  -0.33369532  0.16642703]\n",
      " [-0.3318987   0.16631456  0.16558418]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def crossentropy_softmax_backward_cpu(dlogits, dlosses, probs, targets):\n",
    "    B, V = dlogits.shape\n",
    "    for b in range(B):\n",
    "        dlogits_b = dlogits[b]\n",
    "        probs_b = probs[b]\n",
    "        dloss = dlosses[b]\n",
    "        ix = targets[b]\n",
    "        for i in range(V):\n",
    "            p = probs_b[i]\n",
    "            indicator = 1.0 if i == ix else 0.0\n",
    "            dlogits_b[i] += (p - indicator) * dloss\n",
    "\n",
    "def crossentropy_softmax_backward(dlogits, dlosses, probs, targets):\n",
    "    B = len(targets)\n",
    "    V = len(probs) // B\n",
    "\n",
    "    # Backwards through both softmax and crossentropy\n",
    "    for b in range(B):\n",
    "        for i in range(V):\n",
    "            indicator = 1.0 if i == targets[b] else 0.0\n",
    "            dlogits[i + b * V] += probs[i + b * V]*(probs[i + b * V] - indicator) * dlosses[b]\n",
    "\n",
    "\n",
    "# Input data\n",
    "B = 2\n",
    "V = 3\n",
    "dlogits = torch.zeros(B, V, requires_grad=True)\n",
    "dlosses = torch.tensor([0.1, 0.2])\n",
    "probs = F.softmax(torch.rand(B, V), dim=1)\n",
    "targets = torch.tensor([1, 0])\n",
    "\n",
    "# convert arrays to numpy\n",
    "dlogits_numpy = dlogits.detach().numpy()\n",
    "dlosses_numpy = dlosses.numpy()\n",
    "probs_numpy = probs.detach().numpy()\n",
    "targets_numpy = targets.numpy()\n",
    "\n",
    "# Custom implementation\n",
    "crossentropy_softmax_backward(dlogits_numpy, dlosses_numpy, probs_numpy, targets_numpy)\n",
    "\n",
    "# Compute gradients using PyTorch autograd\n",
    "loss = F.cross_entropy(dlogits, targets)\n",
    "loss.backward()\n",
    "\n",
    "# Compare gradients\n",
    "print(\"Gradients - Custom Implementation:\")\n",
    "print(dlogits.grad)\n",
    "\n",
    "print(\"\\nGradients - PyTorch Autograd:\")\n",
    "print(dlogits.grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      "tensor([[ 0.1281,  0.0192, -0.3127,  0.1316,  0.0338],\n",
      "        [-0.3003,  0.0147,  0.1182,  0.1163,  0.0511],\n",
      "        [ 0.0260,  0.0581,  0.0192,  0.0581, -0.1615]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have your model predictions and ground truth labels\n",
    "predictions = torch.randn(3, 5, requires_grad=True)\n",
    "labels = torch.tensor([2, 0, 4])\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = F.cross_entropy(predictions, labels)\n",
    "\n",
    "# Perform backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "gradients = predictions.grad\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients from PyTorch autograd:\n",
      "tensor([[ 0.0662, -0.2704,  0.0718,  0.0772,  0.0552],\n",
      "        [-0.2763,  0.0400,  0.0960,  0.0481,  0.0922],\n",
      "        [-0.2772,  0.0852,  0.0609,  0.0472,  0.0840]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Validation\n",
    "B = 3  # Batch size\n",
    "V = 4  # Vocabulary size\n",
    "\n",
    "# Random input tensors\n",
    "dlogits = torch.tensor([[0.567248, 0.515976, 0.648122, 0.720573, 0.385052],\n",
    " [0.380139, 0.0264595, 0.901303, 0.209906, 0.861324],\n",
    " [0.545885, 0.964293, 0.628193, 0.372692, 0.949461]]\n",
    ")\n",
    "dlosses = torch.tensor([[-0.0282296, -0.454024, 0.604358, -0.817072, -0.103977],\n",
    " [ 0.280313, 0.946532, 0.321757, 0.581225, 0.12656],\n",
    " [-0.919248, 0.76281, -0.863277, -0.406415, 0.970153]])\n",
    "\n",
    "# probs = torch.softmax(torch.randn(B, V), dim=1)\n",
    "targets = torch.tensor([1,0,0])\n",
    "\n",
    "# PyTorch's autograd requires gradients to be calculated\n",
    "dlogits.requires_grad = True\n",
    "\n",
    "# Compute loss and gradients\n",
    "loss = torch.nn.CrossEntropyLoss()(dlogits, targets)\n",
    "loss.backward()\n",
    "\n",
    "# Print and compare gradients\n",
    "print(\"\\nGradients from PyTorch autograd:\")\n",
    "print(dlogits.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
